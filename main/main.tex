\documentclass[english]{tktltiki}
\usepackage[pdftex]{graphicx}
\usepackage{subfigure}
\usepackage{url}
\begin{document}
%\doublespacing
%\singlespacing
\onehalfspacing

\title{Big Data Influence Maximization blah balh}
\author{Behrouz Derakhshan}
\date{\today}

\maketitle

\numberofpagesinformation{\numberofpages\ pages + \numberofappendixpages\ appendices}
\classification{\protect{\ \\
A.1 [Introductory and Survey],\\
I.7.m [Document and text processing]}}

\keywords{layout, summary, list of references}

\begin{abstract}

Network influence maximization. Something about graphs, about network influcence, social network, other related areas, big data, hadoop and other technologies, graph processing

\end{abstract}

\mytableofcontents




\section{Introduction}
What are graphs, social graphs. Influence maximization in social networks. Its application in marketing.  \cite{kempe03} has defined models in which network propagation can be studied. 
\cite{domingo01} studied how to find the value of a customer in a network and how to use that to influence other customers.




\section{Literature Review}



\subsection{Graph}
%\enlargethispage{5mm}




\subsection{Social Graphs}




\subsection{Influence Maximization}
Influence maximization problem is divided into two major parts. The first one is to find the graph of users (nodes) where edges between each user show the probability of  a user influencing the other. The second part is
based on this graph find a set of users that are proven to have the highest influence over others, in other words 
this limited group of users can maximize the influence through the network.



In \cite{kempe03} they have studied models by which influence propagates through social networks. They have discussed two diffusion models \\
Linear Threshold Model (add reference) and Independent Cascade Models(add reference) . Describe the two models briefly. 
The problem of influence maximization which is finding an initial set that will maximizes the profit( target function) is NP-hard . 
Formal definition : it asks for a parameter k, to find a k-nod set of maximum influence. They used approximation with a natural greedy hill-climbing strategy which will guarantee a 63 percent of the optimal solution performance (reference) . For the approximation to work , he target function should be sub-modular, they have proved that for both models the target function is indeed sub-modular.  They have run experiments on co-authorships in physics theory section of arXiv (www.arxiv.org) . The results show that their greedy algorithm performs better that other methods such as random selection, high-degree or central nodes discussed in book Social Network Analysis by S. Wasserman (find reference) \\
\cite{domingo01} discusses the use case of influence maximization in marketing. They first, described how to model a market as a social network. Making each individual user only dependent on its neighbours, the product and the marketing action(Markov random field). Through these a function was devised that calculates the global lift in profit given a marketing action was applied to some users. Starting from an initial configuration, using different optimization algorithms a local maxima for the function can be reached. 
They experimented on a collaborative filtering dataset (MovieLenz) in which users have rated a list of movies. They have first constructed the network using the user rating, by choosing similar users( calculating the Pearson correlation coefficient of users) as neighbours . They have performed experiments on the data set and compared their method with mass marketing and direct marketing. Their method has increased the profit the most. Due to non-linearly of the model, it did not scale well for bigger size networks, hence they have proposed a new linear model \cite{domingo02}. Not only the new model decreased the computation time time, the simplified equation for calculating the network value of customers, made it easier to incorporate more complex marketing action. They have experimented with Epinions (cite) data set. Which is a website where users can rate items. It also has a feature called trusted users, where each user can select many other users as trusted sources of reviews. Domingo et .al, hence, applied their model to the dataset, but considering trusted users of a user his neighbours. Their experiments showed that a continuous marketing action (a marketing action that can have unlimited values, such as a discount) performs much better than boolean marketing actions (one that can either be true or false, such whether or not a discount should be given to a user with no regards for the amount of the discount ). 
\cite{cheng13} Have addressed both the accuracy and scalability of the solutions using the greedy methods. They have pointed out the main bottle neck in methods based on \cite{kempe03} are in the Monte Carlo simulation steps. They have proven that the objective function is not entirely sub-modular due to the randomness in the simulations, and hence to compensate for that many Monte Carlo simulations have to be run in each step of the algorithm which greatly reduces the performance. Present the proof here ....\\
They proposed a new method called StaticGreedy which uses precomputed a series of snapshots at the beginning of the algorithm and reuses those for every step of the greedy algorithm.  They have defined the two terms, snapshots and simulations which can be both used for the Monte Carlo simulations. \\
\begin{itemize}
\item 
Simulation : The influence spread is obtained by directly simulating the random process of diffusion triggered by a given seed set S.
\item
Snapshot : According to the characteristic of IC(Independent Cascade) model, whether u successfully activates v depends only on p(u,v). Hence prior to the main algorithm a Graph G$'$=(V,E$'$), which is a subgraph of G where an edge <u,v> is remained with the probability p(u,v) and deleted otherwise. 
\end{itemize}
Before the main steps of the algorithm plenty of snapshots are produced and averaged to estimate the spread function. 
Have StaticGreedy algorithm here :\\
1. Static snapshots \\
2. Greedy selection \\

Two main difference between this method and the previous ones are :
\begin{itemize}
\item
Monte Carlo simulations are conducted in static snapshot manner, which are sampled before the greedy process of selecting seed nodes
\item
The same set of snapshots are reused in every iteration to estimate the influence spread I(S), which explains the meaning of ''static''
\end{itemize}

They have performed experiment and compared their methods with the normal Greedy methods. None of the other greedy methods are any match with StaticGreedy in terms of speed and scalability. 

Saito and Kimura \cite{kimura06} have also investigated the scalability issue of ICM simulations. They have argued that the current simulation model in ICM requires many iterations and each iteration took a lot of processing time and power. They have proposed two natural special cases of the ICM such that a good estimate of this quantity can be efficiently computed. They define the Shortest-Path Model(SPM) which is a special case of the ICM such that each node v has the chance to become active only at step = d(A,v) , where if A is the initial set d(A,v) is the distance of A to node v. This means that each node is activated only through the shortest paths from the initial active set. The other model they have proposed named SP1M, which is each node v has the chance to become active only at steps t = d(A,v) and t =d(A,v) + 1. Through these two models they have proposed methods for calculating the spread of an initial set A. They have proved that the result achieved using this method is also within a bound of the best solution. In their experiments, they have assigned a uniform probably to each edge with two different values p = 0.1 and p =0.01. The experiments showed that the estimation of the spread for ICM increases as p increases while the processing times for the SPM and SP1M hardly change. They have also \\



\subsubsection{Learning Influence Probabilities}
\cite{goyal10} have tackled the problem of forming the social graph whose edges have the probability of a user
influencing the other based on a log of actions. Based on the General Threshold model that simultaneously 
generalizes the Linear Threshold and Independent Cascade models. They have formulated the problem like this :
given graph G = (V,E,T) with T being the time when each edge in the graph was constructed, an action log which contains relations Actions(User,Action, Time) which contains tuples indicating a user performed an action at certain time. The aim is to a function p : E --> [0,1] x [0,1] assigning both directions of each edge (u,v) in E the probabilities : $p_{v,u}$ and $p_{u,v}$.
Three different solution models were proposed. Static Model in which all the probabilities remain the same all the time. Continues Time (CT) model where probability at each time step is based on the neighbours formation and Discrete Time which is approximation to CT model since CT model is very resource and time consuming. 
The models can be calculated with 2 scans of the data sets. The probability of v activating u $p_{v,u}$ can be calculated in two ways : 
Bernoulli distribution :  $p_{v,u}$ =  $A_{v2u}$ / ${A_v}$ \\
Jaccard Index : $p_{v,u}$ =  $A_{v2u}$ / $A_{u|v}$ \\ 
These two can also be used with a more sophisticated approach card Partial Credits (PC) which will assign a 
weight to each node based on the size of the active neighborhood . Experimental evaluations show good results 
specially with DT and CT models. CT outperformed DT by a very small amount, but it is much more resource intensive. \\
In another paper, Goyal et al \cite{goyal11} have also tackled the problem of influence maximization directly. Instead of first learning the edge probabilities and then finding the optimum seed set, they have proposed a method to directly find the optimal seed set based on the action log .

\subsubsection{Maximizing Influence through a Network}



\subsubsection{Submodular function optimization}
\cite{Leskovec07} et al. in their Cost effective outbreak detection in networks, took upon the task of optimizing the greedy method used in propagation in graph networks. Although they did not work on solving the problem of maximizing influence they have proven their method can be adapted to the problem of maximizing the influence using the greedy algorithm. They studied two problems from different domains which share similarities. The first problem is find the best placement of sensors in a water network, so that contaminated water can be detected as quickly as possible. The other problem is blog network, which is to find blogs that contains the maximum amount of news that cascades from different sources in shortest possible time. Their advanced greedy method, utilizes the submodularity of the objective function, which reduces the number of computation needed to be made in each iteration. Their method is 700 times faster than a simple greedy function. Details of algorithm can be found here :
PRESENT THE ALGORITHM






\subsection{Big Data and hadoop}


\subsection{Use cases reasearch and industry}



\section{Big data and Network influence maximization}

Description of what I'm going to do, big data/hadoop/mapreduce implementions for network influence


\subsection{Tools}




\subsection{Data set}





\subsection{Algorithm}
\subsubsection{Algorithm that I'm going to user}
\subsubsection{Big data implementaiton of the Algorithm}



\subsection{Other challenges and open questions}



\section{Conclusion}
In conclusion
\pagebreak



\bibliographystyle{tktl}

\bibliography{bibliography}

\lastpage

\appendices

\pagestyle{empty}


\end{document}


