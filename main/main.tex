\documentclass[english]{tktltiki}
\usepackage[pdftex]{graphicx}
\usepackage{subfigure}
\usepackage{url}
\begin{document}
%\doublespacing
%\singlespacing
\onehalfspacing

\title{Big Data Influence Maximization blah balh}
\author{Behrouz Derakhshan}
\date{\today}

\maketitle

\numberofpagesinformation{\numberofpages\ pages + \numberofappendixpages\ appendices}
\classification{\protect{\ \\
A.1 [Introductory and Survey],\\
I.7.m [Document and text processing]}}

\keywords{layout, summary, list of references}

\begin{abstract}

Network influence maximization. Something about graphs, about network influcence, social network, other related areas, big data, hadoop and other technologies, graph processing

\end{abstract}

\mytableofcontents




\section{Introduction}
The term graph, first used by mathematician J. J. Sylvester, is a representation of set of objects, some of which are connected. Typically, in a graph the interconnected components are called \textbf{vertices}, and the links that are connecting them are called edges. Graph theory, is the field of mathematics that studies graphs and since its introduction, it has been used extensively, in many branches of science. In computer science, graphs can represents, networks of computers and the communication between them, it can represent websites and the links between them. In Chemistry and Physics, graphs are used to model the atoms and molecules and their interactions. In sociology, relation between actors, directors and movies can be depicted using a graph. And most notably is the its case in social networks and modeling the relationship between people (friendship, acquaintance) . \\
Graphs components, can carry extra information, for example the edges can carry weight which represent the connection between two nodes. For example in a graph that represent geographical locations, where vertices (nodes) represent places, the edges represent the length between the locations. 
With introduction of online social networks, an enormous amount of information is now available . One application that has been studied in social network or in graphs in general is the spread of information between nodes, based on the structure of the graph and the edge weights. In medicine and biology it has been used to find the spread of a disease either to find the initial person who contracted the disease or finding key vertices that could amplify the spread and by identifying those nodes the further spread of the disease can be avoided. It has use cases in marketing as well, in which by the aim is to find key people in a social network, that can spread the information among their friends and acquaintances, and for example by offering discount to a certain group of people they can rely on the work-of-mouth phenomena. \cite{kempe03} has defined models in which network propagation can be studied, and how to maximizes the the propagation in a network. \cite{domingo01} studied how to find the value of a customer in a network and how to use that to influence other customers. Influence maximization that is the topic of this thesis, has been studied for more than a decade now, but with the massive amount of available data, the scalability has become an issue. Some websites, such as Facebook, Twitter, Amazon, ... carry Terabytes and some Petabytes of data related to their customers and their relationship. And the models developed for simulating the network propagation and finding the initial set that maximizes the influence they will all suffer greatly when applied to this amount of data. \\
Over the past couple of years, many platforms for processing huge amount of data has been developed. This branch of computer science, named BigData, has gain a lot of attention lately, websites can now produce terabytes of data about the users. 
Some of the most notable frameworks are Hadoop and Spark. Hadoop is open source project that is based on Google's MapReduce framework. 
Using these and other similar frameworks, many graph processing tools and applications are built which utilizes the mechanism of these frameworks to process graphs of millions or billion of nodes and trillions of edges. \\
The most widely used frameworks for processing big graphs, are :
\begin{itemize}
\item 
Giraph which is open source project by on Google's Pregel framework. It is built on top of Hadoop
\item 
GraphX is open source project built on top of Spark
\item
Graphlab and open source project written in  and C++ . 
\end{itemize}
In this thesis, I will study the performance penalty of running state of the art algorithms for network propagation, more specifically the Independent Cascade method (which will be explained in the next section) . And propose and implement distributed methods for the algorithm written for GraphLab, GraphX and Giraph and comparison between each frameworks will be made. 
The rest of this document is organized as follow; in section 2, Big Data frameworks, graph frameworks, influence maximization methods are studies. In section 3, a distributed algorithm and implementation methods are presented. In section 4, comparison of the distributed and single node implementations as well as the difference between each graph framework are studied . In section 5 conclusion blah blah
In the next sections, different Big Data platforms are studied in details. Graph processing frameworks are explained extensively. And current and state of the art algorithms for network propagation and influence maximization are studied. 




\section{Literature Review}



\subsection{Graph}
%\enlargethispage{5mm}
\subsection{Graph Databases Mostly copy paste from the Book GraphDatabase}
A graph database is an online (real-time) database management system with Create, Read, Update, and Delete (CRUD) methods that expose a graph data model. Graph databases are generally built for use with transactional (OLTP) systems. Accordingly,they are normally optimized for transactional performance, and engineered with transactional integrity and operational availability in mind.
Two properties of graph databases are useful to understand when investigating graph database technologies :
The underlying storage: \\
Some graph databases use native graph storage, which is optimized and designed for storing and managing graphs. Not all graph database technologies use native graph storage, however. Some serialize the graph data into a relational database, object-oriented databases, or other types of general-purpose data stores.
The processing engine :\\
Some definitions of graph databases require that they be capable of index-free adjacency,2 meaning that connected nodes physically ''point'' to each other in the database. Here we take a slightly broader view: that any database that from the user's perspective behaves like a graph database (i.e., exposes a graph data model through CRUD operations), qualifies as a graph database. We do acknowledge, however, the significant performance advantages of index-free adjacency, and therefore use the term native graph processing in reference to graph databases that leverage index- free adjacency.

Graph Data models : \\
Property Graph : 
1. It contains nodes and relationships
2. nodes contain properties
3. relationships are named and directed
4. relationships can contain properties

Hypergraphs:
where each relationship (edge) can connect any number of nodes. It is generally good when the domain consist of many-to-many relations.

Triplets : 
A triplet is a subject-predicate-object data structure. 

Storage options for graphs.
Relational databases are not good for storing relations. They rely on joins to find the relation between connected data, and the complexity arises for 
querying nested relationship between items. \\
NOSQL databases also are not optimized for storing connected data. Though they usually store fields connecting them to the next item, this value essentially act as a foreign key, hence requiring join operation. They usually tend work well for queries that requires traversing in the direction of links, but reverse the queries will require a brute force scan of the full table. Consider a social network, each user stored with its id as the key, it also holds a field that points to its friends IDs. In this scenario, finding friends of a user is easy, but finding who is friend with a user requires scanning the entire table (Assuming friendship is a one-directional, i.e. one person can consider someone else a friend whereas the other person doesn't consider the first person his/her friend) . Even if each node has another property that holds the id of people who are friend with it, it is still require index look up, something that graph database doesn't require because it provides index-free adjacency, having extra fields cause the disk space to grow, write operations to be more expensive and ... .\\
Graph Databases have the index-free adjacency property, which means that each node has reference to its connecting node instead of the database maintaining a global index.

\subsection{Neo4j }
Neo4j stores graph data in a number of different store files. Each store file contains the data for a specific part of the graph.\\
Node :\\
The node store file stores node records. Every node created in the user-level graph ends up in the node store, the physical file for which is neostore.nodestore.db. Like most of the Neo4j store files, the node store is a fixed-size record store, where each record is nine bytes in length. \\
Relationship :\\
relationships are stored in the relationship store file, neostore.rela tionshipstore.db. Like the node store, the relationship store consists of fixed-sized records in this case each record is 33 bytes long. Each relationship record contains the IDs of the nodes at the start and end of the relationship, a pointer to the relationship type (which is stored in the relationship type store), and pointers for the next and previous relationship records for each of the start and end nodes. These last pointers are part of what is often called the relationship chain.

\subsection{Titan}: 
Titan is an open source Graph Database, that provides several different storage back ends, as well different querying language. As opposed to Neo4j that only supported native storage and native query language. Titan is designed to work well in distributed environment, it has support of Hadoop as a processing tool and uses Apache HBase, Apache Cassandra and Oracle BerkelyDB for storage options. 
Data model of Titan : \\
Titan stores vertices and edges using an adjacency list format. Where the id of each is stored and it points to a list containing all the connected edges and the vertex properties. It follows Google's BigTable format. The design model imposes a series of technical limitations. Some of which are : 
\begin{itemize}
\item 
Titan can store up to a quintillion edges (2 \^ 60) and half as many vertices. That limitation is imposed by Titan's id scheme.
\item
Subclasses cannot be used for data type definitions. For example if a data type is declared as Number one cannot use Long or Integer. 
\item
Edge retrieval is O(log(k)) where k is degree of a edge vertex. However, Titan tries to pick the endpoint with smaller degree.

\end{itemize}

\subsection{FlockDB}
A graph database developed by twitter. It is not a database optimized for graph traversal as opposed to Titan and Neo4j . It is optimized for very large 
adjacency lists, fast read and writes and arithmetic queries. It was designed with these goals in mind :
\begin{itemize}
\item
A high rate of add/update/remove operations
\item
complex set arithmetic operations
\item
paging through query result sets containing millions of entires
\item
ability to archive and later restore archive edges
\item
online data migration
\end{itemize} 

Twitter uses FlockDB to store social graphs and secondary indices. The underlying storage engine is MySQL. Each edge is stored as a row in the database and contains 4 fields :
\begin{itemize}
\item
64 bit source IDopen	
\item
64 bit destination ID
\item
state : normal, removed or archived
\item
32 bit position used for sorting (usually timestamp)
\end{itemize}

\subsection{BigData} 
\subsection{Giraph}
Apache Giraph is an iterative graph processing framework, built on top of Apache Hadoop. It is the open source version of Pregel by Google. 
\subsection{GraphX}
\subsection{Pregel}
Graph processing framework by Google. Pregel is a vertex centric computational model where each vertex process the incoming messages from the edges and pass along the output to the next vertices through edges. It is built for iterative processing and in a distributed environment. The API is similar to MapReduce where the users have to implement methods for two set of methods, the compute method and the aggregate method. Compute methods are executed on each vertex in parallel, there are mechansim involved however, to assert data consistency. The computations are done in steps called SuperStep. The main method is the compute method, where in each SuperStep a vertex will receive a list of messages from vertices in an arbitrary order, and after the processing is done, it will send messages to other vertices. The aggregator can collect global values from all vertices, they are usually  to keep track of some global counts or .... . 
\subsection{GraphLab}
GraphLab is a graph processing framework, tailored for big data sets. There are three versions of GraphLab :
\begin{itemize}
\item
Shared memory, multi-core parallel GraphLab
\item
Distributed GraphLab
\item
PowerGraph

\end{itemize} 










\subsection{Social Graphs}

\subsection{Influence Maximization}
Influence maximization problem is divided into two major parts. The first one is to find the graph of users (nodes) where edges between each user show the probability of  a user influencing the other. The second part is
based on this graph find a set of users that are proven to have the highest influence over others, in other words 
this limited group of users can maximize the influence through the network.



In \cite{kempe03} they have studied models by which influence propagates through social networks. They have discussed two diffusion models \\
Linear Threshold Model (add reference) and Independent Cascade Models(add reference) . Describe the two models briefly. 
The problem of influence maximization which is finding an initial set that will maximizes the profit( target function) is NP-hard . 
Formal definition : it asks for a parameter k, to find a k-nod set of maximum influence. They used approximation with a natural greedy hill-climbing strategy which will guarantee a 63 percent of the optimal solution performance (reference) . For the approximation to work , he target function should be sub-modular, they have proved that for both models the target function is indeed sub-modular.  They have run experiments on co-authorships in physics theory section of arXiv (www.arxiv.org) . The results show that their greedy algorithm performs better that other methods such as random selection, high-degree or central nodes discussed in book Social Network Analysis by S. Wasserman (find reference) \\
\cite{domingo01} discusses the use case of influence maximization in marketing. They first, described how to model a market as a social network. Making each individual user only dependent on its neighbours, the product and the marketing action(Markov random field). Through these a function was devised that calculates the global lift in profit given a marketing action was applied to some users. Starting from an initial configuration, using different optimization algorithms a local maxima for the function can be reached. 
They experimented on a collaborative filtering dataset (MovieLenz) in which users have rated a list of movies. They have first constructed the network using the user rating, by choosing similar users( calculating the Pearson correlation coefficient of users) as neighbours . They have performed experiments on the data set and compared their method with mass marketing and direct marketing. Their method has increased the profit the most. Due to non-linearly of the model, it did not scale well for bigger size networks, hence they have proposed a new linear model \cite{domingo02}. Not only the new model decreased the computation time time, the simplified equation for calculating the network value of customers, made it easier to incorporate more complex marketing action. They have experimented with Epinions (cite) data set. Which is a website where users can rate items. It also has a feature called trusted users, where each user can select many other users as trusted sources of reviews. Domingo et .al, hence, applied their model to the dataset, but considering trusted users of a user his neighbours. Their experiments showed that a continuous marketing action (a marketing action that can have unlimited values, such as a discount) performs much better than boolean marketing actions (one that can either be true or false, such whether or not a discount should be given to a user with no regards for the amount of the discount ). 
\cite{cheng13} Have addressed both the accuracy and scalability of the solutions using the greedy methods. They have pointed out the main bottle neck in methods based on \cite{kempe03} are in the Monte Carlo simulation steps. They have proven that the objective function is not entirely sub-modular due to the randomness in the simulations, and hence to compensate for that many Monte Carlo simulations have to be run in each step of the algorithm which greatly reduces the performance. Present the proof here ....\\
They proposed a new method called StaticGreedy which uses precomputed a series of snapshots at the beginning of the algorithm and reuses those for every step of the greedy algorithm.  They have defined the two terms, snapshots and simulations which can be both used for the Monte Carlo simulations. \\
\begin{itemize}
\item 
Simulation : The influence spread is obtained by directly simulating the random process of diffusion triggered by a given seed set S.
\item
Snapshot : According to the characteristic of IC(Independent Cascade) model, whether u successfully activates v depends only on p(u,v). Hence prior to the main algorithm a Graph G$'$=(V,E$'$), which is a subgraph of G where an edge <u,v> is remained with the probability p(u,v) and deleted otherwise. 
\end{itemize}
Before the main steps of the algorithm plenty of snapshots are produced and averaged to estimate the spread function. 
Have StaticGreedy algorithm here :\\
1. Static snapshots \\
2. Greedy selection \\

Two main difference between this method and the previous ones are :
\begin{itemize}
\item
Monte Carlo simulations are conducted in static snapshot manner, which are sampled before the greedy process of selecting seed nodes
\item
The same set of snapshots are reused in every iteration to estimate the influence spread I(S), which explains the meaning of ''static''
\end{itemize}

They have performed experiment and compared their methods with the normal Greedy methods. None of the other greedy methods are any match with StaticGreedy in terms of speed and scalability. 

Saito and Kimura \cite{kimura06} have also investigated the scalability issue of ICM simulations. They have argued that the current simulation model in ICM requires many iterations and each iteration took a lot of processing time and power. They have proposed two natural special cases of the ICM such that a good estimate of this quantity can be efficiently computed. They define the Shortest-Path Model(SPM) which is a special case of the ICM such that each node v has the chance to become active only at step = d(A,v) , where if A is the initial set d(A,v) is the distance of A to node v. This means that each node is activated only through the shortest paths from the initial active set. The other model they have proposed named SP1M, which is each node v has the chance to become active only at steps t = d(A,v) and t =d(A,v) + 1. Through these two models they have proposed methods for calculating the spread of an initial set A. They have proved that the result achieved using this method is also within a bound of the best solution. In their experiments, they have assigned a uniform probably to each edge with two different values p = 0.1 and p =0.01. The experiments showed that the estimation of the spread for ICM increases as p increases while the processing times for the SPM and SP1M hardly change. They have also \\



\subsubsection{Learning Influence Probabilities}
\cite{goyal10} have tackled the problem of forming the social graph whose edges have the probability of a user
influencing the other based on a log of actions. Based on the General Threshold model that simultaneously 
generalizes the Linear Threshold and Independent Cascade models. They have formulated the problem like this :
given graph G = (V,E,T) with T being the time when each edge in the graph was constructed, an action log which contains relations Actions(User,Action, Time) which contains tuples indicating a user performed an action at certain time. The aim is to a function p : E --> [0,1] x [0,1] assigning both directions of each edge (u,v) in E the probabilities : $p_{v,u}$ and $p_{u,v}$.
Three different solution models were proposed. Static Model in which all the probabilities remain the same all the time. Continues Time (CT) model where probability at each time step is based on the neighbours formation and Discrete Time which is approximation to CT model since CT model is very resource and time consuming. 
The models can be calculated with 2 scans of the data sets. The probability of v activating u $p_{v,u}$ can be calculated in two ways : 
Bernoulli distribution :  $p_{v,u}$ =  $A_{v2u}$ / ${A_v}$ \\
Jaccard Index : $p_{v,u}$ =  $A_{v2u}$ / $A_{u|v}$ \\ 
These two can also be used with a more sophisticated approach card Partial Credits (PC) which will assign a 
weight to each node based on the size of the active neighborhood . Experimental evaluations show good results 
specially with DT and CT models. CT outperformed DT by a very small amount, but it is much more resource intensive. \\
In another paper, Goyal et al \cite{goyal11} have also tackled the problem of influence maximization directly. Instead of first learning the edge probabilities and then finding the optimum seed set, they have proposed a method to directly find the optimal seed set based on the action log .

\subsubsection{Maximizing Influence through a Network}



\subsubsection{Submodular function optimization}
\cite{Leskovec07} et al. in their Cost effective outbreak detection in networks, took upon the task of optimizing the greedy method used in propagation in graph networks. Although they did not work on solving the problem of maximizing influence they have proven their method can be adapted to the problem of maximizing the influence using the greedy algorithm. They studied two problems from different domains which share similarities. The first problem is find the best placement of sensors in a water network, so that contaminated water can be detected as quickly as possible. The other problem is blog network, which is to find blogs that contains the maximum amount of news that cascades from different sources in shortest possible time. Their advanced greedy method, utilizes the submodularity of the objective function, which reduces the number of computation needed to be made in each iteration. Their method is 700 times faster than a simple greedy function. Details of algorithm can be found here :
PRESENT THE ALGORITHM






\subsection{Big Data and hadoop}


\subsection{Use cases reasearch and industry}



\section{Big data and Network influence maximization}

Description of what I'm going to do, big data/hadoop/mapreduce implementions for network influence


\subsection{Tools}




\subsection{Data set}





\subsection{Algorithm}
\subsubsection{Algorithm that I'm going to user}
\subsubsection{Big data implementaiton of the Algorithm}



\subsection{Other challenges and open questions}



\section{Conclusion}
In conclusion
\pagebreak



\bibliographystyle{tktl}

\bibliography{bibliography}

\lastpage

\appendices

\pagestyle{empty}


\end{document}


